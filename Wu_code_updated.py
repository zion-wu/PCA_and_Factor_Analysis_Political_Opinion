# -*- coding: utf-8 -*-
"""MSDS411_Assignment 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E6Babe43azS1mEFnSPY16DFq4ji-4rf3
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from factor_analyzer import calculate_kmo, FactorAnalyzer
import pyreadstat

# Load the SPSS .sav file using pyreadstat
file_path = '/content/Mar19public.sav'
data, meta = pyreadstat.read_sav(file_path)

# Extract column names and labels
column_labels = meta.column_labels
value_labels = meta.value_labels

# Prepare the column descriptions
column_descriptions = [{"Variable Name": var, "Description": label} for var, label in zip(meta.column_names, meta.column_labels)]

# Convert to DataFrame for easier visualization
column_descriptions_df = pd.DataFrame(column_descriptions)

# Display the first few rows of column descriptions
print(column_descriptions_df.head())

# Display the first few rows of the dataset
data.head()

# Summarize the data types and missing values
data.info()
data.isnull().sum()

# Exploratory Data Analysis (EDA)
print("Summary Statistics:")
print(data.describe())

# Identify Variables for Political Opinion
pewopinion = data.iloc[:, 25:98]

pewopinion.info()

# Check rows with complete data across all selected columns
complete_cases = pewopinion.dropna(how="any")
print(f"\nNumber of original cases with complete data: {len(complete_cases)}")
print(f"Number of original cases with incomplete data: {len(pewopinion) - len(complete_cases)}")

# Identify columns where all rows are non-missing
complete_columns = pewopinion.columns[pewopinion.notna().all()]
print(f"\nNumber of items with complete data: {len(complete_columns)}")
print(f"Names of items with complete data: {list(complete_columns)}")

# Create a new DataFrame with only complete columns
pewwork = pewopinion[complete_columns]
print(pewwork.info())

# One-hot encode categorical variables
encoder = OneHotEncoder(sparse_output=False)
design_matrix = encoder.fit_transform(pewwork)

# Convert the design matrix into a DataFrame
data_prepared = pd.DataFrame(
    design_matrix,
    columns=encoder.get_feature_names_out(pewwork.columns)
)
print(data_prepared.info())

# Extract dummy column names and original column names
dummy_columns = data_prepared.columns.tolist()  # Get dummy column names
original_columns = pewwork.columns.tolist()  # Original column names before encoding

# Create mapping from dummy columns to original columns
mapping = {}
for dummy_col in dummy_columns:
    base_col = dummy_col.split('_')[0]  # Extract the original column name (before '_')
    mapping[dummy_col] = base_col

# Convert mapping to a DataFrame for easier handling
mapping_df = pd.DataFrame(list(mapping.items()), columns=["Dummy Column", "Original Column"])

# Principal Component Analysis (PCA)
# Fit PCA to the data
pca = PCA()
pca_result = pca.fit_transform(data_prepared)

# Explained Variance
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

# Visualize Scree Plot
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o', linestyle='--')
plt.title('Scree Plot - Explained Variance by Principal Component')
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.grid()
plt.show()

# Ensure `data` only includes variables used in the analysis
data_for_analysis = data_prepared.select_dtypes(include=[np.number])  # Keep only numeric columns

from factor_analyzer import calculate_kmo, FactorAnalyzer

# Calculate eigenvalues from Factor Analysis
fa = FactorAnalyzer(rotation=None)
fa.fit(data_for_analysis)
eigenvalues, _ = fa.get_eigenvalues()

# Plot Scree Plot for Factor Analysis
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker="o", linestyle="--")
plt.title("Scree Plot - Factor Analysis Eigenvalues")
plt.xlabel("Factors")
plt.ylabel("Eigenvalue")
plt.grid()
plt.show()

# According to the elbow point on the Scree Plot, first 7 factors are mannually chosen
mannual_n_factors = 7

# Run Factor Analysis with the selected number of factors
fa = FactorAnalyzer(n_factors=mannual_n_factors, rotation="varimax")
fa.fit(data_for_analysis)

# Extract and save factor loadings
factor_loadings = pd.DataFrame(
    fa.loadings_,
    index=data_for_analysis.columns,
    columns=[f"Factor{i+1}" for i in range(mannual_n_factors)]
)
print(factor_loadings)

# Merge mapping with column descriptions
mapped_with_descriptions = pd.merge(
    mapping_df,
    column_descriptions_df,
    left_on="Original Column",
    right_on="Variable Name",
    how="left"
)

# Add descriptions back to the factor loadings
factor_loadings_with_desc = factor_loadings.reset_index()  # Reset index for merging
factor_loadings_with_desc = pd.merge(
    factor_loadings_with_desc,
    mapped_with_descriptions,
    left_on="index",  # Merge on dummy column names
    right_on="Dummy Column",
    how="left"
)

# Reorganize columns for readability
factor_loadings_with_desc = factor_loadings_with_desc[[
    "index", "Original Column", "Description", *factor_loadings.columns
]].rename(columns={"index": "Dummy Column"})

# Set threshold for high factor loadings
threshold = 0.4
high_loadings_results = []

# Iterate through factors and extract high loadings with descriptions
for factor in factor_loadings.columns:
    high_loadings = factor_loadings_with_desc[factor].abs() > threshold
    high_loading_vars = factor_loadings_with_desc[high_loadings]
    high_loadings_results.append(high_loading_vars[["Dummy Column", "Description", factor]])
    print(f"\nHigh loading variables for {factor}:")
    print(high_loading_vars[["Dummy Column", "Description", factor]])

high_loadings_df = pd.concat(high_loadings_results, ignore_index=True)
high_loadings_df.to_csv("high_loadings_results.csv", index=False)